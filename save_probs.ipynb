{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import timm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_chans=4\n",
    "out_dim_be=2\n",
    "out_dim_lsk=3\n",
    "drop_rate=0\n",
    "drop_path_rate=0\n",
    "drop_rate_last=0\n",
    "image_size=296\n",
    "backbone_bowel=\"tf_efficientnetv2_b0.in1k\"\n",
    "backbone_lsk=\"tf_efficientnetv2_b2.in1k\"\n",
    "model_bowel_dir='./models_bowel'\n",
    "kernel_bowel_type='0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep'\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        step_dim=x.shape[1]\n",
    "        feature_dim = self.feature_dim\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij) # slice importances\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10 # normalize across slices\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmModelBowel(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False,features=False):\n",
    "        super().__init__()\n",
    "        self.features=features\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=out_dim_be,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        hlstm=64\n",
    "        self.lstm = nn.LSTM(hdim, hlstm, num_layers=1, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2*hlstm, hlstm),\n",
    "            nn.BatchNorm1d(hlstm),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(hlstm, out_dim_be),\n",
    "        )\n",
    "        self.head2=nn.Linear(4*hlstm,out_dim_be)\n",
    "        self.attention=Attention(2*hlstm)\n",
    "\n",
    "    def forward(self, x):  # (bs, nslice, ch, sz, sz)\n",
    "        bs = x.shape[0]\n",
    "        nslices=x.shape[1]\n",
    "        x = x.view(bs * nslices, in_chans, image_size, image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, nslices, -1)\n",
    "        feat, _ = self.lstm(feat)\n",
    "        \n",
    "        featv = feat.contiguous().view(bs * nslices, -1)\n",
    "        out = self.head(featv)\n",
    "        out = out.view(bs, nslices,out_dim_be).contiguous()\n",
    "        \n",
    "        att=self.attention(feat)\n",
    "        max_=feat.max(dim=1)[0]\n",
    "        conc=torch.cat((att,max_),dim=1)\n",
    "        out2=self.head2(conc)\n",
    "\n",
    "\n",
    "        if self.features:\n",
    "            return feat\n",
    "        else:\n",
    "            return out,out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(ifold):\n",
    "    print(ifold)\n",
    "    model_bowel = TimmModelBowel(backbone_bowel, pretrained=False,features=True)\n",
    "    model_bowel_file = os.path.join(model_bowel_dir, f'{kernel_bowel_type}_fold{ifold}_best.pth')\n",
    "    model_bowel.load_state_dict(torch.load(model_bowel_file))\n",
    "    model_bowel = model.to(device)\n",
    "\n",
    "    dataset = CLSDataset(df, 'test', transform=transforms_valid) \n",
    "    save_path=f\"features2b/model{ifold}\"           \n",
    "    os.makedirs(save_path,exist_ok=True)\n",
    "    for ind,row in tqdm(df.iterrows()):\n",
    "        images = dataset[ind]\n",
    "        with torch.no_grad():\n",
    "            images=images.cuda()\n",
    "            images=images[None,...]\n",
    "            features = model(images).squeeze()\n",
    "            features=features.numpy(force=True)\n",
    "        np.save(f\"{save_path}/{row.patient_id}_{row.series_id}\",features)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "PREP_FEATURES=True\n",
    "if PREP_FEATURES:\n",
    "    infer_features(0)\n",
    "    infer_features(1)\n",
    "    infer_features(2)\n",
    "    infer_features(3)\n",
    "    infer_features(4)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
