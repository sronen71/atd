{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T05:59:39.369783Z","iopub.status.busy":"2022-10-29T05:59:39.369067Z","iopub.status.idle":"2022-10-29T06:00:25.730464Z","shell.execute_reply":"2022-10-29T06:00:25.729271Z","shell.execute_reply.started":"2022-10-29T05:59:39.369678Z"},"trusted":true},"outputs":[],"source":["!pip -q install monai\n","!pip -q install segmentation-models-pytorch\n","\n","!pip -q install /kaggle/input/pylibjpeg140py3/pylibjpeg-1.4.0-py3-none-any.whl\n","!pip -q install /kaggle/input/pylibjpeg140py3/python_gdcm-3.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-29T06:00:25.733795Z","iopub.status.busy":"2022-10-29T06:00:25.733350Z","iopub.status.idle":"2022-10-29T06:00:25.741619Z","shell.execute_reply":"2022-10-29T06:00:25.740579Z","shell.execute_reply.started":"2022-10-29T06:00:25.733741Z"},"trusted":true},"outputs":[],"source":["DEBUG = False\n","\n","import sys\n","sys.path = [\n","    '/kaggle/input/covn3d-same',\n","] + sys.path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:25.743219Z","iopub.status.busy":"2022-10-29T06:00:25.742950Z","iopub.status.idle":"2022-10-29T06:00:34.162024Z","shell.execute_reply":"2022-10-29T06:00:34.160905Z","shell.execute_reply.started":"2022-10-29T06:00:25.743184Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import cv2\n","import time\n","import timm\n","import pydicom\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import nibabel as nib\n","from tqdm import tqdm\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","from torch.utils.data import Dataset\n","\n","from monai.transforms import Resize\n","import  monai.transforms as transforms\n","\n","%matplotlib inline\n","rcParams['figure.figsize'] = 20, 8\n","device = torch.device('cuda')\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:29.485211Z","iopub.status.busy":"2022-10-29T06:03:29.484820Z","iopub.status.idle":"2022-10-29T06:03:29.494317Z","shell.execute_reply":"2022-10-29T06:03:29.493294Z","shell.execute_reply.started":"2022-10-29T06:03:29.485168Z"},"trusted":true},"outputs":[],"source":["kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n","load_kernel = None\n","n_folds = 5\n","\n","\n","image_seg_size = [128, 128, 128]\n","\n","\n","init_lr = 3e-3\n","batch_size = 1\n","drop_rate = 0.\n","drop_path_rate = 0.\n","loss_weights = [1, 1]\n","p_mixup = 0.1\n","\n","data_dir = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\n","use_amp = True\n","num_workers = 4\n","out_dim = 5\n","\n","n_epochs = 100\n","\n","log_dir = './logs'\n","model_dir_seg = './models_seg'\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(model_dir_seg, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.390438Z","iopub.status.busy":"2022-10-29T06:00:34.389599Z","iopub.status.idle":"2022-10-29T06:00:34.402322Z","shell.execute_reply":"2022-10-29T06:00:34.401400Z","shell.execute_reply.started":"2022-10-29T06:00:34.390400Z"},"trusted":true},"outputs":[],"source":["keys=(\"image\",\"mask\")\n","interpolation_modes = ('bilinear', 'nearest') \n","transforms_train = transforms.Compose([\n","    #transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1), \n","    #transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2), \n","    transforms.RandAffined(keys=keys, \n","                           rotate_range=(0.2,0.2,0.2), # radians\n","                           scale_range=(0.2,0.2,0.2),\n","                           translate_range=[int(x*y) for x, y in zip(image_seg_size, [0.1, 0.1, 0.2])], \n","                           padding_mode='zeros',\n","                           prob=0.7,\n","                           mode=interpolation_modes,\n","                           ),\n","    transforms.RandGridDistortiond(keys=keys, prob=0.5, distort_limit=(-0.03, 0.03), mode=interpolation_modes),    \n","])\n","\n","transforms_valid = transforms.Compose([\n","])"]},{"cell_type":"markdown","metadata":{},"source":["# DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.404497Z","iopub.status.busy":"2022-10-29T06:00:34.403952Z","iopub.status.idle":"2022-10-29T06:00:34.486417Z","shell.execute_reply":"2022-10-29T06:00:34.485281Z","shell.execute_reply.started":"2022-10-29T06:00:34.404459Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(os.path.join(data_dir,'train_series_meta.csv'))\n","\n","mask_files = os.listdir(f'{data_dir}/segmentations')\n","df_mask = pd.DataFrame({\n","    'mask_file': mask_files,\n","})\n","df_mask['series_id'] = (df_mask['mask_file'].apply(lambda x: x[:-4]))\n","df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir,'segmentations', x))\n","df_mask['series_id']=df_mask['series_id'].astype(np.int64)\n","df = df_train.merge(df_mask, on='series_id', how='left')\n","df['image_folder'] = data_dir+\"/train_images/\"+df['patient_id'].astype(str)+\"/\"+df[\"series_id\"].astype(str)\n","df['mask_file'].fillna('', inplace=True)\n","\n","df_seg = df.query('mask_file != \"\"').reset_index(drop=True)\n","\n","kf = KFold(5,shuffle=True)\n","df_seg['fold'] = -1\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg, df_seg)):\n","    df_seg.loc[valid_idx, 'fold'] = fold\n","\n","df_seg.tail()"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.497353Z","iopub.status.busy":"2022-10-29T06:00:34.496904Z","iopub.status.idle":"2022-10-29T06:00:34.514198Z","shell.execute_reply":"2022-10-29T06:00:34.513162Z","shell.execute_reply.started":"2022-10-29T06:00:34.497317Z"},"trusted":true},"outputs":[],"source":["def load_dicom_2d(path,resize=None): #image_seg_size\n","    dicom = pydicom.read_file(path)\n","    data = dicom.pixel_array\n","    if dicom.PixelRepresentation == 1:\n","        bit_shift = dicom.BitsAllocated - dicom.BitsStored\n","        dtype = data.dtype \n","        data = (data << bit_shift).astype(dtype) >>  bit_shift\n","    data = pydicom.pixel_data_handlers.util.apply_modality_lut(data, dicom) \n","    if resize:\n","        data = cv2.resize(data, (resize[0], resize[1]), interpolation = cv2.INTER_LINEAR)\n","    orient=dicom.ImageOrientationPatient\n","    pos=dicom.ImagePositionPatient\n","    data=np.transpose(data,[1,0])\n","\n","    return data,orient,pos\n","\n","def window_images(images,img_min = -300,img_max = 400): \n","    images[images < img_min] = img_min\n","    images[images > img_max] = img_max\n","    \n","    # normalization\n","    images = (images - img_min)/(img_max - img_min)\n","    return (images * 255).astype(np.uint8)\n","\n","\n","def check_orient(paths):\n","    dicom1=pydicom.dcmread(paths[0],specific_tags=[\"ImageOrientationPatient\",\"ImagePositionPatient\"])\n","\n","    dicom2=pydicom.dcmread(paths[-1],specific_tags=[\"ImageOrientationPatient\",\"ImagePositionPatient\"])\n","    orient=dicom1.ImageOrientationPatient\n","    positions=[dicom1.ImagePositionPatient,dicom2.ImagePositionPatient]\n","    imaging_axis=np.cross(orient[:3],orient[3:])\n","    dot = np.dot(np.array(positions[1])-np.array(positions[0]), imaging_axis)\n","    return dot\n","\n","def stack_images(images,orient,positions):\n","    \n","    #make sure images are ordered correctly (superior is z+) \n","    imaging_axis=np.cross(orient[:3],orient[3:])\n","    distance_projection = np.dot(np.stack(positions), imaging_axis)\n","    images = np.stack(images, -1)\n","    images = images[:,:,np.argsort(distance_projection)]\n","    return images\n","\n","\n","def load_dicom_3d(paths,resize_2d=None):\n","\n","    images = []\n","    positions=[]\n","    for filename in paths:\n","        img,orient,pos=load_dicom_2d(filename,resize=resize_2d)\n","        images.append(img)\n","        positions.append(pos)    \n"," \n","    images=stack_images(images,orient,positions)\n","    images = window_images(images)\n","    return images\n","\n","def load_dicom_folder(folder,num,resize_2d=None):\n","    t_paths = sorted(glob(os.path.join(folder, \"*\")),\n","                     key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n","    n_scans = len(t_paths)\n","    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., num)).round().astype(int)\n","    t_paths = [t_paths[i] for i in indices]\n","    return load_dicom_3d(t_paths,resize_2d=resize_2d)\n","\n","\n","\n","def load_nii_mask(mask_file,size):\n","    nib_mask= nib.load(mask_file)\n","    mask=np.asanyarray(nib_mask.dataobj).astype(np.uint8)\n","    mask=mask[:,::-1,:]\n","    mask = Resize(size,mode=\"nearest\")(mask[None,...]).numpy().squeeze()\n","    mask = mask.astype(np.uint8)\n","    return mask\n","\n","\n","def load_image_mask(image_folder, mask_file,resize=image_seg_size):\n","\n","    image = load_dicom_folder(image_folder,num=resize[2],resize_2d=resize[:2])\n","   \n","    if mask_file is not None:\n","        mask=load_nii_mask(mask_file,resize)\n","    else:\n","        mask=None\n","    \n","    return image, mask\n","    \n","\n","def load_sample(row, has_mask=True):\n","    mask_file=None\n","    if has_mask:\n","        mask_file=row.mask_file\n","    image,mask=load_image_mask(row.image_folder,mask_file)\n","    if has_mask:\n","        return image,mask\n","    else:\n","        return image\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask_file=\"/kaggle/input/rsna-2023-abdominal-trauma-detection/segmentations/10000.nii\"\n","image_folder=\"/kaggle/input/rsna-2023-abdominal-trauma-detection/train_images/54722/10000\"\n","image,mask=load_image_mask(image_folder,mask_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib.colors import ListedColormap\n","#rcParams['figure.figsize'] = 20,8\n","label_dict = {\n","    0: 'background',\n","    1: 'liver',\n","    2: 'spleen',\n","    3: 'left kidney',\n","    4: 'right kidney',\n","    5: 'bowel'\n","}\n","\n","color_dict = {\n","    0: 'black',\n","    1: 'blue',\n","    2: 'green',\n","    3: 'wheat',\n","    4: 'pink',\n","    5: 'brown'\n","}\n","\n","cm = ListedColormap(color_dict.values())\n","def visualize(image,mask=None,title=None,alpha=0.4):\n","    image=image[:,::-1,:]\n","    slice1=image.shape[2]//2\n","    slice2=image.shape[1]//2\n","    slice3=image.shape[0]//2\n","\n","    fig,ax=plt.subplots(1,3)\n","    ax[0].imshow(image[:,:,slice1].T,cmap=\"gray\",origin=\"lower\")\n","    ax[1].imshow(image[:,slice2,:].T,cmap=\"gray\",origin=\"lower\")\n","    ax[2].imshow(image[slice3,:,:].T,cmap=\"gray\",origin=\"lower\")\n","    if mask is not None:\n","        mask=mask[:,::-1,:]\n","        ax[0].imshow(mask[:,:,slice1].T,alpha=alpha,origin=\"lower\",cmap=cm,interpolation='nearest')\n","        ax[1].imshow(mask[:,slice2,:].T,alpha=alpha,origin=\"lower\",cmap=cm,interpolation='nearest')\n","        ax[2].imshow(mask[slice3,:,:].T,alpha=alpha,origin=\"lower\",cmap=cm,interpolation='nearest')\n","    fig.suptitle(title)\n","visualize(image,mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["res = transforms_train({'image': np.expand_dims(image, 0).repeat(3, 0), 'mask':mask[None,...]})\n","im,ma=res[\"image\"],res[\"mask\"]\n","ma=ma.squeeze().to(torch.int)\n","im=im[0,...]\n","im=im.numpy(force=True)\n","ma=ma.numpy(force=True)\n","visualize(im,ma)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessed1_dir=\"preprocessed1\"\n","def preprocess(df,dry_run=False):\n","    os.makedirs(preprocessed1_dir,exist_ok=True)\n","    for ind,row in df.iterrows():\n","        #print(ind,end=\" \")\n","        mask_file=None\n","        if row.mask_file!=\"\":\n","            mask_file=row.mask_file\n","        image_folder=row.image_folder\n","        print(ind,image_folder,mask_file)\n","        image,mask=load_image_mask(image_folder,mask_file)\n","        if not dry_run:\n","            save_image_file = os.path.join(preprocessed1_dir, f'{row.series_id}.npy')\n","            np.save(save_image_file,image)\n","            if mask_file:\n","                save_mask_file = os.path.join(preprocessed1_dir,f'{row.series_id}_mask.npy')\n","                np.save(save_mask_file,mask)\n","\n","PREPROCESS=False\n","if PREPROCESS:\n","    preprocess(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class SEGDataset(Dataset):\n","    def __init__(self, df, mode, transform=None,use_preprocessed=True):\n","\n","        self.df = df\n","        self.mode = mode\n","        self.transform = transform\n","        self.use_preprocessed=use_preprocessed\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        is_test=(self.mode==\"test\") # in test mode we don't load masks, just predict them\n","        \n","        if self.use_preprocessed:\n","            image_file = os.path.join(preprocessed1_dir, f'{row.series_id}.npy')\n","            image = np.load(image_file)\n","            if not is_test:\n","                mask_file = os.path.join(preprocessed1_dir, f'{row.series_id}_mask.npy')\n","                mask = np.load(mask_file).astype(np.uint8)\n","        else:\n","            image, mask = load_sample(row, has_mask= (not is_test))\n","\n","        image=torch.from_numpy(image)\n","        image=image[None,...]\n","        if not is_test:\n","            mask=torch.from_numpy(mask)\n","\n","        if self.transform:\n","            res = self.transform({'image':image, 'mask':mask[None,...]})   \n","            image = res['image'] \n","            mask = res['mask']\n","      \n","        image=image/255.\n","        if is_test:\n","            return image\n","        else:\n","            mask=mask.squeeze().to(torch.long)\n","            return image, mask\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.516237Z","iopub.status.busy":"2022-10-29T06:00:34.515883Z","iopub.status.idle":"2022-10-29T06:00:34.528793Z","shell.execute_reply":"2022-10-29T06:00:34.527571Z","shell.execute_reply.started":"2022-10-29T06:00:34.516189Z"},"trusted":true},"outputs":[],"source":["\n","\n","dataset_show = SEGDataset(df_seg, 'train', transform=transforms_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:00:34.533948Z","iopub.status.busy":"2022-10-29T06:00:34.533491Z","iopub.status.idle":"2022-10-29T06:02:59.056109Z","shell.execute_reply":"2022-10-29T06:02:59.055183Z","shell.execute_reply.started":"2022-10-29T06:00:34.533918Z"},"trusted":true},"outputs":[],"source":["rcParams['figure.figsize'] = 20,8\n","for i in range(3):\n","    img, mask = dataset_show[i]\n","    img=img[0,...]\n","    img=img.numpy(force=True)\n","    mask=mask.numpy(force=True)\n","    visualize(img,mask)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.059903Z","iopub.status.busy":"2022-10-29T06:02:59.058663Z","iopub.status.idle":"2022-10-29T06:02:59.070787Z","shell.execute_reply":"2022-10-29T06:02:59.069705Z","shell.execute_reply.started":"2022-10-29T06:02:59.059860Z"},"trusted":true},"outputs":[],"source":["class TimmSegModel(nn.Module):\n","    def __init__(self, backbone='resnet18d', segtype='unet', out_dim=6,pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            drop_rate=drop_rate,\n","            drop_path_rate=drop_path_rate,\n","            pretrained=pretrained\n","        )\n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        n_blocks=4\n","        if segtype == 'unet':\n","            self.decoder = smp.decoders.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.n_blocks=n_blocks\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:self.n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:02:59.073051Z","iopub.status.busy":"2022-10-29T06:02:59.072642Z","iopub.status.idle":"2022-10-29T06:03:13.720715Z","shell.execute_reply":"2022-10-29T06:03:13.719595Z","shell.execute_reply.started":"2022-10-29T06:02:59.073017Z"},"trusted":true},"outputs":[],"source":["from timm.layers.conv2d_same import Conv2dSame\n","from conv3d_same import Conv3dSame\n","\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output\n","\n","\n","m_2d = TimmSegModel()\n","m = convert_3d(m_2d)\n","m(torch.rand(1, 1, 128,128,128)).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Loss & Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.724951Z","iopub.status.busy":"2022-10-29T06:03:13.724580Z","iopub.status.idle":"2022-10-29T06:03:13.736570Z","shell.execute_reply":"2022-10-29T06:03:13.735400Z","shell.execute_reply.started":"2022-10-29T06:03:13.724920Z"},"trusted":true},"outputs":[],"source":["\n","def binary_dice_score(\n","    y_pred: torch.Tensor,\n","    y_true: torch.Tensor,\n","    eps: float = 1e-7,\n",") -> float:\n","    \n","    if (torch.sum(y_true)==0) and (torch.sum(y_pred)==0):\n","        return (torch.sum(y_true)==0).to(torch.float32)\n","    else:\n","        intersection = torch.sum(y_pred * y_true)\n","        cardinality = torch.sum(y_pred) + torch.sum(y_true)\n","        score = (2.0 * intersection) / (cardinality + eps)\n","        return score\n","\n","\n","def multilabel_dice_score(\n","    y_true: torch.Tensor, # single image, no batch\n","    y_pred: torch.Tensor,\n","    eps=1e-7,\n","    num_classes=6 # including background\n","):\n","    ious = []\n","    for class_index in range(1,num_classes): #ignore background\n","        y_pred1=(y_pred==class_index).to(torch.float32)\n","        y_true1=(y_true==class_index).to(torch.float32)\n","        iou = binary_dice_score(\n","            y_pred1,\n","            y_true1,\n","            eps=eps,\n","        )\n","        ious.append(iou)\n","        \n","    ious=torch.stack(ious)\n","    return ious\n","\n","\n","def dice_loss(input, target,num_classes=6,do_one_hot=True):\n","    input = torch.nn.Softmax(dim=1)(input)\n","    smooth = 1.0\n","    iflat = input.view(-1)\n","    if do_one_hot:\n","        t1hot=F.one_hot(target,num_classes)\n","    else:\n","        t1hot=target\n","    tflat = t1hot.view(-1)\n","    intersection = (iflat * tflat).sum()\n","    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n","\n","\n","def ce_dice(input, target, loss_weights=loss_weights):\n","    #loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n","    loss1=loss_weights[0]*nn.CrossEntropyLoss()(input,target)\n","    loss2 = loss_weights[1] * dice_loss(input, target)\n","    return (loss1 + loss2) / sum(loss_weights)\n","\n","criterion = ce_dice"]},{"cell_type":"markdown","metadata":{},"source":["# Train & Valid func"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.738907Z","iopub.status.busy":"2022-10-29T06:03:13.738511Z","iopub.status.idle":"2022-10-29T06:03:13.755925Z","shell.execute_reply":"2022-10-29T06:03:13.755044Z","shell.execute_reply.started":"2022-10-29T06:03:13.738871Z"},"trusted":true},"outputs":[],"source":["def mixup(input, truth, clip=[0, 1]):\n","    indices = torch.randperm(input.size(0))\n","    shuffled_input = input[indices]\n","    shuffled_labels = truth[indices]\n","\n","    lam = np.random.uniform(clip[0], clip[1])\n","    input = input * lam + shuffled_input * (1 - lam)\n","    return input, truth, shuffled_labels, lam\n","\n","\n","def train_func(model, loader_train, optimizer, scaler=None):\n","    model.train()\n","    train_loss = []\n","    bar = tqdm(loader_train)\n","    for images, gt_masks in bar:\n","        optimizer.zero_grad()\n","        images = images.cuda()\n","        gt_masks = gt_masks.cuda()\n","\n","        #do_mixup = False\n","        #if random.random() < p_mixup:\n","        #    do_mixup = True\n","        #    images, gt_masks, gt_masks_sfl, lam = mixup(images, gt_masks)\n","\n","        with amp.autocast():\n","            logits = model(images)\n","            loss = criterion(logits, gt_masks)\n","            #if do_mixup:\n","            #    loss2 = criterion(logits, gt_masks_sfl)\n","            #    loss = loss * lam  + loss2 * (1 - lam)\n","\n","        train_loss.append(loss.item())\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n","\n","    return np.mean(train_loss)\n","\n","\n","def valid_func(model, loader_valid):\n","    model.eval()\n","    valid_loss = []\n","    batch_metrics = [] \n","    bar = tqdm(loader_valid)\n","    with torch.no_grad():\n","        for images, gt_masks in bar:\n","            images = images.cuda()\n","            gt_masks = gt_masks.cuda()\n","            with amp.autocast():\n","                logits = model(images)\n","                loss = criterion(logits, gt_masks)\n","            valid_loss.append(loss.item())\n","            for i in range(logits.shape[0]): # iterate over batch\n","                y_pred=torch.argmax(logits[i],dim=0)\n","                y_true=gt_masks[i] \n","                tmp = multilabel_dice_score(y_pred,y_true)\n","                batch_metrics.append(tmp)\n","            bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n","    batch_metrics=torch.stack(batch_metrics,dim=0)        \n","    metrics = torch.mean(batch_metrics,axis=0)\n","    metrics=metrics.numpy(force=True)\n","    print('dc:', metrics,\"mean\",np.mean(metrics))\n","\n","    return np.mean(valid_loss), np.mean(metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.757646Z","iopub.status.busy":"2022-10-29T06:03:13.757188Z","iopub.status.idle":"2022-10-29T06:03:13.977069Z","shell.execute_reply":"2022-10-29T06:03:13.976109Z","shell.execute_reply.started":"2022-10-29T06:03:13.757610Z"},"trusted":true},"outputs":[],"source":["\n","#rcParams['figure.figsize'] = 20, 2\n","optimizer = optim.AdamW(m.parameters(), lr=init_lr)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n","lrs = []\n","for epoch in range(1, n_epochs+1):\n","    scheduler_cosine.step(epoch-1)\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","plt.plot(range(len(lrs)), lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:13.979257Z","iopub.status.busy":"2022-10-29T06:03:13.978679Z","iopub.status.idle":"2022-10-29T06:03:13.992437Z","shell.execute_reply":"2022-10-29T06:03:13.991378Z","shell.execute_reply.started":"2022-10-29T06:03:13.979202Z"},"trusted":true},"outputs":[],"source":["def run(fold):\n","\n","    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n","    model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n","\n","    train_ = df_seg[df_seg['fold'] != fold].reset_index(drop=True)\n","    valid_ = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n","    dataset_train = SEGDataset(train_, 'train', transform=transforms_train)\n","    dataset_valid = SEGDataset(valid_, 'valid', transform=transforms_valid)\n","    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    model = TimmSegModel(pretrained=True)\n","    model = convert_3d(model)\n","    model = model.to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n","    scaler = torch.cuda.amp.GradScaler()\n","    metric_best = 0.\n","\n","    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs)\n","\n","    print(len(dataset_train), len(dataset_valid))\n","\n","    for epoch in range(1, n_epochs+1):\n","        scheduler_cosine.step(epoch-1)\n","\n","        print(time.ctime(), 'Epoch:', epoch)\n","\n","        train_loss = train_func(model, loader_train, optimizer, scaler)\n","        valid_loss, metric = valid_func(model, loader_valid)\n","\n","        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n","        print(content)\n","        with open(log_file, 'a') as appender:\n","            appender.write(content + '\\n')\n","\n","        if metric > metric_best:\n","            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n","            torch.save(model.state_dict(), model_file)\n","            metric_best = metric\n","\n","        # Save Last\n","        if not DEBUG:\n","            torch.save(\n","                {\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n","                    'score_best': metric_best,\n","                },\n","                model_file.replace('_best', '_last')\n","            )\n","\n","    del model\n","    torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-29T06:03:33.006143Z","iopub.status.busy":"2022-10-29T06:03:33.005515Z"},"trusted":true},"outputs":[],"source":["TRAIN_STAGE1=False\n","if TRAIN_STAGE1:\n","    run(0)\n","    run(1)\n","    run(2)\n","    run(3)\n","    run(4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models_seg=[]\n","for fold in range(5):\n","    model = TimmSegModel(pretrained=False)\n","    model = convert_3d(model)\n","    model = model.to(device)\n","    load_model_file = os.path.join(model_dir_seg, f'{kernel_type}_fold{fold}_best.pth')\n","    sd = torch.load(load_model_file)\n","    if 'model_state_dict' in sd.keys():\n","        sd = sd['model_state_dict']\n","    sd = {k[7:] if k.startswith('module.') else k: sd[k] for k in sd.keys()}\n","    model.load_state_dict(sd, strict=True)\n","    model.eval()\n","    models_seg.append(model)\n","\n","len(models_seg)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def infer_masks(df):\n","    for ind,row in df.iterrows():\n","        mask_file=None\n","        if row.mask_file!=\"\":\n","            continue \n","        print(ind,row.series_id)\n","        \n","        image1,_=load_image_mask(row.image_folder,None)\n","        image=(image1/255.).astype(np.float32)\n","        save_mask_file = os.path.join(preprocessed1_dir,f'{row.series_id}_mask.npy')\n","        \n","        with torch.no_grad():\n","            image=torch.from_numpy(image).cuda()\n","            image=image[None,None,...]\n","            pred_masks = []\n","            for model in models_seg:\n","                model.eval()\n","                with amp.autocast():\n","                    pmask = model(image).squeeze().float()\n","                pmask=nn.Softmax(dim=0)(pmask)\n","                pred_masks.append(pmask)\n","        mask = torch.stack(pred_masks, 0).mean(0)\n","        mask=torch.argmax(mask,dim=0)\n","        mask=mask.numpy(force=True).astype(np.uint8)\n","        #visualize(image1,mask)\n","        np.save(save_mask_file,mask)\n","\n","PREP_MASKS=False\n","if PREP_MASKS:\n","    infer_masks(df)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocess  Stage 2 crops"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#df_p=pd.read_csv(data_dir+\"/train.csv\")\n","#df_healthy=df_p[df_p[\"any_injury\"]==0].sample(758)\n","#df_injured=df_p[(df_p[\"kidney_healthy\"]==0) | (df_p[\"liver_healthy\"]==0)| (df_p[\"spleen_healthy\"]==0) | (df_p[\"bowel_healthy\"]==0)]\n","#print(len(df_injured),len(df_healthy))\n","#df1=pd.concat((df_healthy,df_injured))\n","#df_sample=df.merge(df1,on=\"patient_id\")\n","#df_sample[[\"kidney_healthy\",\"liver_healthy\",\"spleen_healthy\",\"bowel_healthy\"]].mean()#\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import threading\n","batch_size_seg=1\n","\n","\n","dataset_seg=SEGDataset(df,mode=\"valid\",use_preprocessed=True)\n","\n","loader_seg = torch.utils.data.DataLoader(dataset_seg, batch_size=batch_size_seg, shuffle=False, num_workers=num_workers)\n","mask_size = image_seg_size[2]\n","n_slice_per_c = [24,24,12,12,128] # liver,spleen,left kidney, right kidney,bowel (all)\n","n_ch=3\n","\n","image_size_cls = 296"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_organ(mask, cid, t_paths, cropped_images,selected_slices):\n","    n_scans = len(t_paths)\n","    organ = []\n","    if cid<4: #liver,spleen,kindey left and right\n","        mask1 = (mask == (cid+1))\n","    else:\n","        mask1=mask # bowel -> extend to all\n","    \n","\n","    n_slice=n_slice_per_c[cid]\n","    dot =check_orient(t_paths)\n","    if dot<0:\n","        t_paths=t_paths[::-1]\n","    \n","    if np.sum(mask1)>0:\n","        x = np.where(mask1.sum(1).sum(1)>0)[0]\n","        y = np.where(mask1.sum(0).sum(1)>0)[0]\n","        z = np.where(mask1.sum(0).sum(0)>0)[0]\n","        x1, x2 = max(0, x[0] - 1), min(mask1.shape[0], x[-1] + 1)\n","        y1, y2 = max(0, y[0] - 1), min(mask1.shape[1], y[-1] + 1)\n","        z1, z2 = max(0, z[0] - 1), min(mask1.shape[2], z[-1] + 1)\n","\n","        zz1, zz2 = int(z1 / mask_size * n_scans), int(z2 / mask_size * n_scans) # in original\n","        inds = np.linspace(zz1 ,zz2-1 ,n_slice).astype(int) # in original\n","        inds_ = np.linspace(z1 ,z2-1 ,n_slice).astype(int) # in mask\n","\n","        #select_paths=[t_paths[i] for i in inds]\n","        #image=load_dicom_3d(select_paths)\n","        #overlay=cv2.resize(mask[:,:,inds_],(512,512),interpolation=cv2.INTER_NEAREST)\n","        #print(label_dict[cid+1])\n","        #visualize(image,overlay)\n","\n","        selected_slices[cid]=[int(t_paths[i].split('/')[-1].split('.')[0]) for i in inds]\n","        for (ind, ind_) in zip(inds, inds_):\n","            mask_this = mask1[:, :, ind_]\n","            istart=ind-n_ch//2\n","            iend=istart+n_ch\n","            istart1=max(istart,0)\n","            iend1=min(iend,n_scans)\n","            select_paths=[t_paths[i] for i in range(istart1,iend1)]\n","         \n","            image=load_dicom_3d(select_paths) # without resizing \n","            if istart<0:\n","                #print(\"istart\",istart)\n","                pad=np.zeros((image.shape[0], image.shape[1],-istart),dtype=np.uint8)\n","                image=np.concatenate((pad,image),axis=2)\n","            if iend>iend1:\n","                #print(\"iend\",iend,iend1)\n","                pad=np.zeros((image.shape[0],image.shape[1],iend-iend1),dtype=np.uint8)\n","                image=np.concatenate((image,pad),axis=2)\n","            \n","            mask_this = mask_this[x1:x2, y1:y2]\n","            xx1 = int(x1 / mask_size * image.shape[0])\n","            xx2 = int(x2 / mask_size * image.shape[0])\n","            yy1 = int(y1 / mask_size * image.shape[1])\n","            yy2 = int(y2 / mask_size * image.shape[1])\n","            image = image[xx1:xx2, yy1:yy2]\n","            image = cv2.resize(image, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n","\n","            if cid < 4:\n","                mask_this = mask_this * 255\n","            else:\n","                mask_this = mask_this * 32\n","\n","            mask_this = mask_this.astype(np.uint8)\n","\n","            mask_this = cv2.resize(mask_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_NEAREST)\n","            #print(image.shape,mask_this.shape)\n","            image = np.concatenate([image, mask_this[:, :, np.newaxis]], -1)\n","            organ.append(image)\n","        organ=np.stack(organ,0) #(n_slice, H,W,channel)\n","\n","    else:\n","        #print(\"organ not found\",label_dict[cid+1])\n","        organ=np.zeros((n_slice_per_c[cid],image_size_cls,image_size_cls,n_ch+1))\n","        selected_slices[cid]=None\n","\n","    organ= np.transpose(organ,(0,3, 1, 2)) # (n_slice, channels,H,W) \n","    VIS=False\n","    if VIS:\n","        orgvis=np.transpose(organ,(1,2,3,0))\n","        vis=orgvis[n_ch//2,...]\n","        ma=orgvis[-1,...]/255\n","        visualize(vis,ma,title=label_dict[cid+1]) \n","\n","    #organ=organ.float()/255.\n","    cropped_images[cid]=organ\n","\n","\n","def load_cropped_images(msk, image_folder):\n","    threads = [None] * 5\n","    cropped_images = [None] * 5 \n","    selected_slices = [None] * 5\n","\n","    t_paths = sorted(glob(os.path.join(image_folder, \"*\")), key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n","    for cid in range(5):\n","        #load_organ(msk,cid,t_paths,cropped_images)\n","        threads[cid] = threading.Thread(target=load_organ, args=(msk, cid, t_paths, cropped_images,selected_slices))\n","        threads[cid].start()\n","    for cid in range(5):\n","        threads[cid].join()\n","    cropped_images=np.concatenate(cropped_images,axis=0)    \n","    return cropped_images, selected_slices\n"]},{"cell_type":"markdown","metadata":{},"source":["# Save preprocessed "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","bar = tqdm(loader_seg)\n","cls_crops_folder=\"preprocessed2\"\n","bowel_crops_folder=\"bowel\"\n","df_image=pd.read_csv(\"/kaggle/input/rsna-2023-abdominal-trauma-detection/image_level_labels.csv\")\n","with torch.no_grad():\n","    for batch_id, (images,masks) in tqdm(enumerate(bar)):\n","        \n","        # SEG\n","        #images = images.cuda()\n","        #pred_masks = []\n","        #for model in models_seg:\n","        #    pmask = model(images)\n","        #    pmask=nn.Softmax(dim=1)(pmask)\n","        #    pred_masks.append(pmask)\n","        #pred_masks = torch.stack(pred_masks, 0).mean(0).cpu().numpy()\n","        #pred_masks=torch.argmax(pred_masks,dim=0)\n","        pred_masks=masks.numpy(force=True)\n","\n","        for i in range(pred_masks.shape[0]):\n","            ind=batch_id*batch_size_seg+i\n","            row = df.iloc[ind]\n","            #print(ind,row.patient_id,row.series_id)\n","            cropped_images,instance_numbers = load_cropped_images(pred_masks[i,...], row.image_folder)\n","            nlsk=sum(n_slice_per_c[:4])\n","            lsk=cropped_images[:nlsk,...]\n","            bowel=cropped_images[nlsk:,...]\n","            #print(cropped_images.shape)\n","            \n","            #np.savez_compressed(f'{cls_crops_folder}/{row.patient_id}_{row.series_id}',lsk)\n","            ibowel_injury=df_image[(df_image[\"patient_id\"]==row.patient_id) & (df_image[\"series_id\"]==row.series_id) & (df_image[\"injury_name\"]==\"Bowel\")][\"instance_number\"].tolist()\n","            iextravasation_injury=df_image[(df_image[\"patient_id\"]==row.patient_id) & (df_image[\"series_id\"]==row.series_id) & (df_image[\"injury_name\"]==\"Active_Extravasation\")][\"instance_number\"].tolist()\n","            instance_bowel=np.array(instance_numbers[4]) # 4 -> bowel \n","            ibowel=np.isin(instance_bowel,ibowel_injury).astype(np.uint8)\n","            iextravasation=np.isin(instance_bowel,iextravasation_injury).astype(np.uint8)\n","            b=np.stack((ibowel,iextravasation),axis=1)\n","            #if len(ibowel_injury)>0:\n","            #    print(\"bowel injury\")\n","            #    print(len(ibowel_injury),np.sum(b))\n","            #assert not (len(ibowel_injury)>0 and np.sum(b)==0)\n","            np.savez_compressed(f'{bowel_crops_folder}/{row.patient_id}_{row.series_id}',bowel,b)\n","            \n","            gc.collect()\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
